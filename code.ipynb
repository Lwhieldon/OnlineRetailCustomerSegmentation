{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\lwhieldon\\OneDrive - SC&H Group\\Administrative\\School Materials\\DATA 602 - Into to Data Analysis and ML\\OnlineRetailCustomerSegmentation\\images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial = pd.read_excel(dataset,encoding=\"ISO-8859-1\",converters={'CustomerID':str,'InvoiceNo':str})\n",
    "df_initial.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this purpose of the exercise, let's first check to see if invoices beginning with 'c' (i.e. cancelled purchases) are in the dataset. Giftware shop is interested in determining customer segmentation based on customers who purchased products, not cancelled purchases, so we can remove all cancelled purchases from the dataset.\n",
    "\n",
    "Let's also remove null records, especially where a customer id is not assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial = df_initial[~df_initial.InvoiceNo.str.startswith('C', na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial.isnull().sum()\n",
    "#Description & Customer ID contain null records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_customerIDs = df_initial['CustomerID'].isnull().sum()/df_initial['CustomerID'].count()\n",
    "\n",
    "print('Percentage of Customer IDs missing is {:0%}'.format(missing_customerIDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since ~34% of the records are missing and still leaves us with a good portion of data, let's remove these records for now\n",
    "#Now we are left with fully populated data. \n",
    "df_initial.dropna(inplace=True)\n",
    "df_initial.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a new Total Invoice Amt Column\n",
    "df_initial['Total Line Amount']= df_initial['Quantity'] * df_initial['UnitPrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a Day of Week Purchase Column\n",
    "df_initial['PurchaseDayofWeek'] = pd.to_datetime(df_initial['InvoiceDate']).dt.dayofweek\n",
    "df_initial['PurchaseDayofWeek'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a Time of Day Purchase Column\n",
    "df_initial['PurchaseTimeofDay'] = pd.to_datetime(df_initial['InvoiceDate'],format='%H:%m').dt.hour\n",
    "df_initial['PurchaseTimeofDay'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a Date Only Column from Invoice Date (no timestamp)\n",
    "df_initial['InvoiceDate_noTime'] = pd.to_datetime(df_initial['InvoiceDate']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total no. of customers: ',df_initial['CustomerID'].nunique())\n",
    "print('Total transactions : ',df_initial['InvoiceNo'].nunique())\n",
    "print('Products sold are : ',df_initial['StockCode'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are looking at the data on a customer level, let's aggregate & perform feature engineering on the dataset:\n",
    "\n",
    "1. Total Purchase Amount by Customer \n",
    "1. Number of Transactons by Customer\n",
    "1. Distinct Products Types Purchased by Customer\n",
    "1. Tenure (in days) of being a customer (based on last transaction in the dataset)\n",
    "1. Apply getdummies to country feature; 1 = UK, 0 = Non-UK\n",
    "1. Customers who, on average, purchase more products during the business week (Monday - Friday). Will denote with 1\n",
    "1. Customers who, on average, purchase more products during normal business hours (To keep our analysis consistent, let's assume the dates and times are collected in British time; therefore, we will check to see times between 9a-5pm British time). Also, we will also factor in when a customer calls during the weekend as I want to make this indicator mutually exclusive from the workweek attribute above. Will denote with 1\n",
    "\n",
    "Once we have aggregated our measures, let's create a new dataframe that groups all features by customer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total Purchase Amount by Customer\n",
    "total_amount = df_initial['Total Line Amount'].groupby(df_initial['CustomerID']).sum().reset_index()\\\n",
    "        .rename(columns={\"Total Line Amount\": \"TotlAmtbyCust\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Transactions by Customer\n",
    "transactions = df_initial['InvoiceNo'].groupby(df_initial['CustomerID']).count().reset_index()\\\n",
    "        .rename(columns={\"InvoiceNo\": \"TotalCustTransactions\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distinct Products Types Purchased by Customer\n",
    "product_types = df_initial['StockCode'].groupby(df_initial['CustomerID']).nunique().reset_index()\\\n",
    "                .rename(columns={\"StockCode\":\"DistProductsbyCust\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tenure (in days) of being a customer (based on last transaction in the dataset)\n",
    "final = df_initial['InvoiceDate_noTime'].max()\n",
    "df_initial['TenureofCustomer'] = final - df_initial['InvoiceDate_noTime']\n",
    "CustomerTenure = df_initial['TenureofCustomer'].groupby(df_initial['CustomerID']).min().dt.days                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_initial['Country'].value_counts()\n",
    "#Biggest customer is from the UK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a good portion of the online retailer's customers are from the UK, let's create a new feature where we assign a customer from the UK with 1 and everyone else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Apply getdummies to country feature\n",
    "Countries = pd.concat([df_initial['CustomerID'],pd.get_dummies(df_initial['Country']=='United Kingdom', prefix='IsUK')],1)\n",
    "Countries = Countries.loc[1:].groupby(Countries['CustomerID']).max()\n",
    "Countries = pd.DataFrame(Countries).reset_index(drop=True)\n",
    "Countries.drop('IsUK_False',axis='columns', inplace=True)\n",
    "Countries=Countries.rename(columns={'IsUK_True':'IsUK'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customers who, on average, purchase more products during the business week (Monday - Friday). Will denote with 1\n",
    "IsWeekday = pd.concat([df_initial['CustomerID'].groupby(df_initial['CustomerID'])\\\n",
    "                       .max(),round(df_initial['PurchaseDayofWeek'].groupby(df_initial['CustomerID']).sum()\\\n",
    "                                    /df_initial['PurchaseDayofWeek'].groupby(df_initial['CustomerID']).count())],1)\n",
    "IsWeekday = pd.DataFrame(IsWeekday).reset_index(drop=True)\n",
    "IsWeekday['IsWeekDay'] = (IsWeekday['PurchaseDayofWeek']==0) & (IsWeekday['PurchaseDayofWeek']==6).any()\n",
    "IsWeekday['IsWeekDay']=pd.get_dummies(IsWeekday['IsWeekDay'])\n",
    "IsWeekday.drop('PurchaseDayofWeek',axis='columns', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customers who, on average, purchase more products during normal business hours (9a-5pm British time). Will denote with 1\n",
    "IsWorkDay = pd.concat([df_initial['CustomerID'].groupby(df_initial['CustomerID'])\\\n",
    "                       .max(),round(df_initial['PurchaseTimeofDay'].groupby(df_initial['CustomerID']).sum()\\\n",
    "                                    /df_initial['PurchaseTimeofDay'].groupby(df_initial['CustomerID']).count())],1)\n",
    "IsWorkDay = pd.DataFrame(IsWorkDay).reset_index(drop=True)\n",
    "\n",
    "IsWorkDay['IsWorkDay'] = np.where((IsWorkDay['PurchaseTimeofDay']>= 9) & (IsWorkDay['PurchaseTimeofDay']<= 14), False, True)\n",
    "IsWorkDay['IsWorkDay']=pd.get_dummies(IsWorkDay['IsWorkDay'])\n",
    "IsWorkDay.drop('PurchaseTimeofDay',axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.merge(total_amount,transactions,how='inner',on='CustomerID')\n",
    "df_new = pd.merge(df_new,product_types,how='inner',on='CustomerID')\n",
    "df_new = pd.merge(df_new,CustomerTenure,how='inner',on='CustomerID')\n",
    "df_new = pd.merge(df_new,Countries,how='inner',on='CustomerID')\n",
    "df_new = pd.merge(df_new,IsWeekday,how='inner',on='CustomerID')\n",
    "df_new = pd.merge(df_new,IsWorkDay,how='inner',on='CustomerID')\n",
    "df_new.drop('CustomerID',axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Analysis of Amount #ofTransactions, and Disinct Products & Tenure\n",
    "\n",
    "attributes = ['TotlAmtbyCust','TotalCustTransactions','DistProductsbyCust','TenureofCustomer']\n",
    "plt.rcParams['figure.figsize'] = [10,8]\n",
    "sns.boxplot(data = df_new[attributes], orient=\"v\", palette=\"Set2\" ,whis=1.5,saturation=1, width=0.7)\n",
    "plt.title(\"Outliers Variable Distribution\", fontsize = 14, fontweight = 'bold')\n",
    "plt.ylabel(\"Range\", fontweight = 'bold')\n",
    "plt.xlabel(\"Attributes\", fontweight = 'bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa! BIG outliers in TotlAmtbyCust; this could throw off our model. Let's reduce the dataset to only include prices & customer transactions that fall with the 95% percentile to help reduce the <b>outliers</b>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_low = df_new[\"TotlAmtbyCust\"].quantile(0.04)\n",
    "q_hi  = df_new[\"TotlAmtbyCust\"].quantile(0.95)\n",
    "df_new = df_new[(df_new[\"TotlAmtbyCust\"] < q_hi) & (df_new[\"TotlAmtbyCust\"] > q_low)]\n",
    "q_low = df_new[\"TotalCustTransactions\"].quantile(0.04)\n",
    "q_hi  = df_new[\"TotalCustTransactions\"].quantile(0.95)\n",
    "df_new = df_new[(df_new[\"TotalCustTransactions\"] < q_hi) & (df_new[\"TotalCustTransactions\"] > q_low)]\n",
    "df_new.reset_index(drop=True, inplace=True)\n",
    "df_new.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an aggregated dataset with features important to customers & removed outliers, let's perform some EDA & visualize the dataset.\n",
    "\n",
    "Let's first look at the continuous features to see if we can obtain any details related to customer spending habits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1 , figsize = (10,15))\n",
    "n = 0 \n",
    "for x in ['TotlAmtbyCust' , 'TotalCustTransactions' ,'DistProductsbyCust','TenureofCustomer']:\n",
    "    n += 1\n",
    "    plt.subplot(3,2,n)\n",
    "    plt.hist(df_new[x] , bins = 100)\n",
    "    plt.title('Histogram of {}'.format(x))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1 , figsize = (15,6))\n",
    "n = 0 \n",
    "for x in ['IsUK' , 'IsWeekDay' ,'IsWorkDay']:\n",
    "    n += 1\n",
    "    plt.subplot(1,3,n)\n",
    "    plt.subplots_adjust(hspace = 0.5 , wspace = 0.5)\n",
    "    sns.countplot(df_new[x] )\n",
    "    plt.title('Histogram of {}'.format(x))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Majority of customers have purchased between \\\\$0-\\\\$1000 in total from Giftware Shop\n",
    "1. Majority of customers have purchased between 0-50 or so times\n",
    "1. Customers tend to only purchase less than 50 distinct products from Giftware Shop in total\n",
    "1. The tenure is a little bit more spread out - But the majority of Giftware Shop's customer base fall between 0 (new) to 100 days of transactions. This might be a good item to note to Giftware Shop that they should focus on retention of customers.\n",
    "1. As expected, the majority of the customers are from the UK\n",
    "1. Most customers purchase products during the week\n",
    "1. Most customers purchase products during the work day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know that most of the customers are in the UK, let's go ahead and remove anyone outside of the UK as a customer. There's not a ton of data to delineate customers from outside of the UK so let's focus this to reflect only UK customers.\n",
    "\n",
    "Let's keep customers who purchase during the week & workday for now since they do not appear to have much presence in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new[(df_new[\"IsUK\"] == 1) ]\n",
    "df_new.reset_index(drop=True, inplace=True)\n",
    "df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_scaled = sc.fit_transform(df_new[['TotlAmtbyCust' , 'TotalCustTransactions' ,'DistProductsbyCust','TenureofCustomer']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import sys\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "kmeans = KMeans(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = KElbowVisualizer(kmeans, k=(2,10), metric = 'calinski_harabasz', timings=False)\n",
    "visualizer.fit(X_scaled) \n",
    "visualizer.show()        \n",
    "visualizer = KElbowVisualizer(kmeans, k=(2,10), metric = 'silhouette', timings=False,locate_elbow=True)\n",
    "visualizer.fit(X_scaled)   \n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means = KMeans(n_clusters= 3, init='k-means++', random_state=0).fit(X_scaled)\n",
    "clusters = k_means.predict(X_scaled)\n",
    "\n",
    "cl_labels_k = k_means.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use PCA to reduce our dimensionality to see how KMeans clusters our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "\n",
    "plt.scatter(X_pca[cl_labels_k==0,0],X_pca[cl_labels_k==0,1],label='Cluster 1')\n",
    "plt.scatter(X_pca[cl_labels_k==1,0],X_pca[cl_labels_k==1,1],label='Cluster 2')\n",
    "plt.scatter(X_pca[cl_labels_k==2,0],X_pca[cl_labels_k==2,1],label='Cluster 3')\n",
    "\n",
    "plt.xlabel('Feature-1')\n",
    "plt.ylabel('Feature-2')\n",
    "plt.legend()\n",
    "plt.title('PCA KMeans Clustering Scatter Plot')\n",
    "plt.savefig('KMEANS.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.DataFrame(k_means.labels_ , columns = list(['labels']))\n",
    "df_labels.astype({'labels': 'object'}).dtypes\n",
    "df_labels.loc[(df_labels.labels == 0)] = 'Cluster 1'\n",
    "df_labels.loc[(df_labels.labels == 1)] = 'Cluster 2'\n",
    "df_labels.loc[(df_labels.labels == 2)] = 'Cluster 3'\n",
    "df_new['Kmeans_labels'] = df_labels['labels'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['Kmeans_labels'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(1 , figsize = (15,15))\n",
    "n = 0 \n",
    "for var in ['TotlAmtbyCust' , 'TotalCustTransactions' ,'DistProductsbyCust','TenureofCustomer']:\n",
    "    n += 1\n",
    "    label = df_new['Kmeans_labels'].unique()                   \n",
    "\n",
    "    plt.subplot(3,2,n)\n",
    "    plt.hist([df_new.loc[df_new['Kmeans_labels'] == x, var] for x in label], label=label)\n",
    "\n",
    "    plt.title('K Means Histogram of {}'.format(var))\n",
    "    plt.legend()\n",
    "plt.suptitle('K Means Cluster Results')\n",
    "plt.savefig('KMEANSHistograms.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions on K-Means Clustering with k=3\n",
    "\n",
    "K-Means Clustering identified 3 primary clusters in the dataset:\n",
    "\n",
    "- Customers tagged to __Cluster Id 2__ have a high amount of transactions as compared to the other customer clusters.\n",
    "- Customers tagged to __Cluster Id 2__ are also frequent buyers & tend to buy a wider array of products.\n",
    "- Customers tagged to __Cluster Id 3__ are newer buyers and, while they buy more than cluster 1, they don't really spend that much from a total amount & transaction perspective.\n",
    "- Customers tagged to __Cluster Id 1__ are not recent buyers and tend to purchase less (in total amount spent, transactions, as well as the type of products they buy) and hence least of importance from business point of view.\n",
    "\n",
    "But based on the dimensionality reduction we perform to visualize our clusters, does this really reflect our customer base? It looks like one big cluster, which gives the indication that KMeans isn't do the best job at fitting our model. Let's try DBSCAN & Spectral Clustering to see if they do a better job at identifying our customer segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "db = DBSCAN().fit(X_scaled)\n",
    "cl_labels_d = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "# X_pca_centers_dbscan = pca.transform(cl_centers)\n",
    "\n",
    "plt.scatter(X_pca[cl_labels_d==0,0],X_pca[cl_labels_d==0,1],label='Cluster 1')\n",
    "plt.scatter(X_pca[cl_labels_d==1,0],X_pca[cl_labels_d==1,1],label='Cluster 2')\n",
    "plt.scatter(X_pca[cl_labels_d==2,0],X_pca[cl_labels_d==2,1],label='Cluster 3')\n",
    "plt.scatter(X_pca[cl_labels_d==-1,0],X_pca[cl_labels_d==-1,1],s=6,label='Noisy')\n",
    "# plt.scatter(X_pca_centers_kmeans[:, 0], X_pca_centers_kmeans[:, 1], c='black', s = 75)\n",
    "plt.xlabel('Feature-1')\n",
    "plt.ylabel('Feature-2')\n",
    "plt.legend()\n",
    "plt.title('PCA DBScan Scatter Plot')\n",
    "plt.savefig('DBSCAN.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.DataFrame(cl_labels_d, columns = list(['labels']))\n",
    "df_labels.astype({'labels': 'object'}).dtypes\n",
    "df_labels.loc[(df_labels.labels == -1)] = 'Noisy'\n",
    "df_labels.loc[(df_labels.labels == 0)] = 'Cluster 1'\n",
    "df_labels.loc[(df_labels.labels == 1)] = 'Cluster 2'\n",
    "df_labels.loc[(df_labels.labels == 2)] = 'Cluster 3'\n",
    "df_new['DBSCAN_labels'] = df_labels['labels'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['DBSCAN_labels'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1 , figsize = (15,15))\n",
    "n = 0 \n",
    "for var in ['TotlAmtbyCust' , 'TotalCustTransactions' ,'DistProductsbyCust','TenureofCustomer']:\n",
    "    n += 1\n",
    "    label = df_new['DBSCAN_labels'].unique()                   \n",
    "\n",
    "    plt.subplot(3,2,n)\n",
    "    plt.hist([df_new.loc[df_new['DBSCAN_labels'] == x, var] for x in label], label=label)\n",
    "\n",
    "    plt.title('DBSCAN Label Histogram of {}'.format(var))\n",
    "    plt.legend()\n",
    "plt.suptitle('DBSCAN Cluster Results')\n",
    "plt.savefig('DBSCANHistograms.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions on DBSCAN Clustering\n",
    "\n",
    "This algorithm looks promising! It was able to identify that the majority of our customers fit in the same cluster based on their behaviors.\n",
    "\n",
    "DBSCAN Clustering identified 3 primary cluster & 1 noisy label:\n",
    "\n",
    "- DBSCAN grouped the majority of our customers in __Cluster Id 1__: It recognized that most of the customers spend and transact relatively in the same manner. This aligns with the 'single' cluster presented in the lower dimensionality visualization.\n",
    "- Customers tagged to __Cluster Id 2__ & __Cluster Id 3__ are more frequent buyers & tend to buy a wider array of products.\n",
    "- We can disregard the __Noisy__ label as the algorithm detected noise in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "sc = SpectralClustering(n_clusters=3).fit(X_scaled)\n",
    "cl_labels_s = sc.labels_\n",
    "np.unique(cl_labels_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "# X_pca_centers_dbscan = pca.transform(cl_centers)\n",
    "\n",
    "plt.scatter(X_pca[cl_labels_s==0,0],X_pca[cl_labels_s==0,1],label='Cluster 1')\n",
    "plt.scatter(X_pca[cl_labels_s==1,0],X_pca[cl_labels_s==1,1],label='Cluster 2')\n",
    "plt.scatter(X_pca[cl_labels_s==2,0],X_pca[cl_labels_s==2,1],label='Cluster 3')\n",
    "# plt.scatter(X_pca_centers_kmeans[:, 0], X_pca_centers_kmeans[:, 1], c='black', s = 75)\n",
    "plt.xlabel('Feature-1')\n",
    "plt.ylabel('Feature-2')\n",
    "plt.title('PCA Spectral Clustering Scatter Plot')\n",
    "plt.legend()\n",
    "plt.savefig('SPECTRALCLUSTER.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.DataFrame(cl_labels_s , columns = list(['labels']))\n",
    "df_labels.astype({'labels': 'object'}).dtypes\n",
    "df_labels.loc[(df_labels.labels == 0)] = 'Cluster 1'\n",
    "df_labels.loc[(df_labels.labels == 1)] = 'Cluster 2'\n",
    "df_labels.loc[(df_labels.labels == 2)] = 'Cluster 3'\n",
    "df_new['SpectralClustering_labels'] = df_labels['labels'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['SpectralClustering_labels'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1 , figsize = (15,15))\n",
    "n = 0 \n",
    "for var in ['TotlAmtbyCust' , 'TotalCustTransactions' ,'DistProductsbyCust','TenureofCustomer']:\n",
    "    n += 1\n",
    "    label = df_new['SpectralClustering_labels'].unique()                   \n",
    "\n",
    "    plt.subplot(3,2,n)\n",
    "    plt.hist([df_new.loc[df_new['SpectralClustering_labels'] == x, var] for x in label], label=label)\n",
    "\n",
    "    plt.title('Spectral Clustering Histogram of {}'.format(var))\n",
    "    plt.legend()\n",
    "plt.suptitle('Spectral Clustering Cluster Results')\n",
    "plt.savefig('SCHistograms.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions on Spectral Clustering\n",
    "\n",
    "Spectral Clustering appears to be the 'middle ground' between KMeans & DBSCAN:\n",
    "- Similar to DBSCAN, Spectral Clustering identified that majority of customers have similar behaviors and grouped them into a single cluster\n",
    "- Like KMeans, it still segregated customers into __Cluster 2__ & __Cluster 3__ for those groups that tend to be more frequent buyers & tend to buy a wider array of products. \n",
    "- Another interesting observation is __Cluster 1__ tends to have less transactions, product selection, and purchases than that of __Cluster 2__ & __Cluster 3__. Seems like Giftware could really benefit from a marketing campaign to bring it new clients!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Observations\n",
    "\n",
    "When we reduce the dimensionality of the data to visualize the clusters, DBSCAN identified a single cluster, aligning to the majority of customers transacting in the same manner. Another important consideration is that all 3 models identified that a good portion of customers tend to only purchase minimally & not buy a ton of products. Perhaps Giftware Shop should ignite customer engagement by conducting additional marketing compaigns to bring more traffic (and thereby purchases) to the online retail site. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
